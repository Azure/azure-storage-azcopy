schedules:
  - cron: "0 0 * * *"
    displayName: Daily Midnight AzCopy Build
    branches:
      include:
        - master
    always: true

trigger:
  branches:
    include:
      - dev

jobs:
  - job: Build
    # allow maximum build time, in case we have build congestion
    timeoutInMinutes: 360
    strategy:
      matrix:
        Ubuntu-20:
          imageName: 'ubuntu-20.04'
          type: 'linux'
        Windows:
          imageName: 'windows-2019'
          type: 'windows'
        MacOS:
          imageName: 'macOS-10.14'
          type: 'mac-os'
    pool:
      vmImage: $(imageName)
    variables:
      - groups: AzCopySecrets
      - name: isMutexSet
          value: 'false'

    steps:
      - task: GoTool@0
        inputs:
          version: '1.15'
      - script: |
          go build -o "$(Build.ArtifactStagingDirectory)/azcopy_linux_amd64"
          go build -tags "se_integration" -o "$(Build.ArtifactStagingDirectory)/azcopy_linux_se_amd64"
          cp NOTICE.txt $(Build.ArtifactStagingDirectory)
        displayName: 'Generate build linux'
        condition: eq(variables.type, 'linux')

      - script: |
          go build -o "$(Build.ArtifactStagingDirectory)/azcopy_windows_amd64.exe"
          go build -o "$(Build.ArtifactStagingDirectory)/azcopy_windows_386.exe"
          cp NOTICE.txt $(Build.ArtifactStagingDirectory)
        displayName: 'Generate build windows'
        condition: eq(variables.type, 'windows')

      - script: |
          go build -o "$(Build.ArtifactStagingDirectory)/azcopy_darwin_amd64"
        displayName: 'Generate build MacOS'
        condition: eq(variables.type, 'mac-os')

      - task: UsePythonVersion@0
        name: 'Set_up_Python'
        inputs:
          versionSpec: '3.7'
      - task: GoTool@0
        name: 'Set_up_Golang'
        inputs:
          version: '1.15'
      - task: DownloadSecureFile@1
        name: ciGCSServiceAccountKey
        displayName: 'Download GCS Service Account Key'
        inputs:
          secureFile: 'ci-gcs-dev.json'
      - script: |
          pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
          pip install azure-core==1.0.0b3
          # acquire the mutex before running live tests to avoid conflicts
          python ./tool_distributed_mutex.py lock "$(MUTEX_URL)"
          # set the variable to indicate that the mutex was actually acquired
          echo '##vso[task.setvariable variable=isMutexSet]true'
        name: 'Acquire_the_distributed_mutex'
      - script: |
          # run unit test and build executable
          # the set -e line is needed so that the unit tests failure would cause the job to fail properly
          # "-check.v" (must be after package list) outputs timings
          set -e
          go test -timeout 45m -race -short -cover ./cmd ./common ./common/parallel ./ste ./azbfs ./sddl "-check.v"
          go build -o azcopy_linux_amd64
        name: 'Run_unit_tests'
        env:
          ACCOUNT_NAME: $(ACCOUNT_NAME)
          ACCOUNT_KEY: $(ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
          AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
      - script: |
          set -e
          go build -o azcopy_linux_amd64
          export AZCOPY_E2E_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          go test -timeout 20m -race -short -cover ./e2etest
        name: 'Run_e2e_tests'
        env:
          AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
          CPK_ENCRYPTION_KEY: $(CPK_ENCRYPTION_KEY)
          CPK_ENCRYPTION_KEY_SHA256: $(CPK_ENCRYPTION_KEY_SHA256)
      - script: |
          go build -o test-validator ./testSuite/
          mkdir test-temp
          export AZCOPY_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          export TEST_SUITE_EXECUTABLE_LOCATION=$(pwd)/test-validator
          export TEST_DIRECTORY_PATH=$(pwd)/test-temp

          # install the CLFSLoad extension
          pip3 install clfsload

          keyctl session test python ./testSuite/scripts/run.py
        name: 'Run_smoke_tests'
        env:
          ACCOUNT_NAME: $(ACCOUNT_NAME)
          ACCOUNT_KEY: $(ACCOUNT_KEY)
          AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
          GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          ACTIVE_DIRECTORY_APPLICATION_ID: $(ACTIVE_DIRECTORY_APPLICATION_ID)
          AZCOPY_SPA_CLIENT_SECRET: $(AZCOPY_SPA_CLIENT_SECRET)
          CONTAINER_OAUTH_URL: $(CONTAINER_OAUTH_URL)
          CONTAINER_OAUTH_VALIDATE_SAS_URL: $(CONTAINER_OAUTH_VALIDATE_SAS_URL)
          CONTAINER_SAS_URL: $(CONTAINER_SAS_URL)
          FILESYSTEM_SAS_URL: $(FILESYSTEM_SAS_URL)
          FILESYSTEM_URL: $(FILESYSTEM_URL)
          OAUTH_AAD_ENDPOINT: $(OAUTH_AAD_ENDPOINT)
          OAUTH_TENANT_ID: $(OAUTH_TENANT_ID)
          PREMIUM_CONTAINER_SAS_URL: $(PREMIUM_CONTAINER_SAS_URL)
          S2S_DST_BLOB_ACCOUNT_SAS_URL: $(S2S_DST_BLOB_ACCOUNT_SAS_URL)
          S2S_SRC_BLOB_ACCOUNT_SAS_URL: $(S2S_SRC_BLOB_ACCOUNT_SAS_URL)
          S2S_SRC_FILE_ACCOUNT_SAS_URL: $(S2S_SRC_FILE_ACCOUNT_SAS_URL)
          S2S_SRC_S3_SERVICE_URL: $(S2S_SRC_S3_SERVICE_URL)
          S2S_SRC_GCP_SERVICE_URL: $(S2S_SRC_GCP_SERVICE_URL)
          SHARE_SAS_URL: $(SHARE_SAS_URL)
      - script: |
          pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
          pip install azure-core==1.0.0b3
          python ./tool_distributed_mutex.py unlock "$(MUTEX_URL)"
        name: 'Release_the_distributed_mutex'
        # this runs even if the job was canceled (only if the mutex was acquired by this job)
        condition: and(always(), eq(variables['isMutexSet'], 'true'))
