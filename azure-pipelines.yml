trigger:
  branches:
    include:
      - dev
      - master

pr:
  branches:
    include:
      - dev
      - master

jobs:
  - job: Linux_and_Windows_Builds
    pool:
      vmImage: 'ubuntu-16.04'
    steps:
      - task: GoTool@0
        inputs:
          version: '1.13'
      - script: |
          GOARCH=amd64 GOOS=linux go build -o "$(Build.ArtifactStagingDirectory)/azcopy_linux_amd64"
          GOARCH=amd64 GOOS=linux go build -tags "se_integration" -o "$(Build.ArtifactStagingDirectory)/azcopy_linux_se_amd64"
          GOARCH=amd64 GOOS=windows go build -o "$(Build.ArtifactStagingDirectory)/azcopy_windows_amd64.exe"
          GOARCH=386 GOOS=windows go build -o "$(Build.ArtifactStagingDirectory)/azcopy_windows_386.exe"
          cp ThirdPartyNotice.txt $(Build.ArtifactStagingDirectory)
        displayName: 'Generate builds'

      - task: PublishBuildArtifacts@1
        displayName: 'Publish Artifacts'
        condition: succeededOrFailed()

  - job: MacOS_Build
    pool:
      vmImage: 'macOS-10.14'
    steps:
      - task: GoTool@0
        inputs:
          version: '1.13'
      - script: |
          go build -o "$(Build.ArtifactStagingDirectory)/azcopy_darwin_amd64"
        displayName: 'Generate builds'

      - task: PublishBuildArtifacts@1
        displayName: 'Publish Artifacts'
        condition: succeededOrFailed()

  - job: Test_On_Ubuntu
    variables:
      isMutexSet: 'false'
    # allow maximum build time, in case we have build congestion
    timeoutInMinutes: 360
    pool:
      vmImage: 'ubuntu-16.04'
    steps:
      - task: UsePythonVersion@0
        name: 'Set_up_Python'
        inputs:
          versionSpec: '3.7'
      - task: GoTool@0
        name: 'Set_up_Golang'
        inputs:
          version: '1.13'
      - task: DownloadSecureFile@1
        name: ciGCSServiceAccountKey
        displayName: 'Download GCS Service Account Key'
        inputs:
          secureFile: 'ci-gcs-dev.json'
      - script: |
          pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
          pip install azure-core==1.0.0b3
          # acquire the mutex before running live tests to avoid conflicts
          python ./tool_distributed_mutex.py lock "$(MUTEX_URL)"
          # set the variable to indicate that the mutex was actually acquired
          echo '##vso[task.setvariable variable=isMutexSet]true'
        name: 'Acquire_the_distributed_mutex'
      - script: |
          # run unit test and build executable
          # the set -e line is needed so that the unit tests failure would cause the job to fail properly
          # "-check.v" (must be after package list) outputs timings
          set -e
          go test -timeout 35m -race -short -cover ./cmd ./common ./common/parallel ./ste ./azbfs ./sddl "-check.v"
          GOARCH=amd64 GOOS=linux go build -o azcopy_linux_amd64
        name: 'Run_unit_tests'
        env:
          ACCOUNT_NAME: $(ACCOUNT_NAME)
          ACCOUNT_KEY: $(ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
          AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
      - script: |
          set -e
          GOARCH=amd64 GOOS=linux go build -o azcopy_linux_amd64
          export AZCOPY_E2E_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          go test -timeout 20m -race -short -cover ./e2etest         
        name: 'Run_e2e_tests'
        env:
          AZCOPY_E2E_ACCOUNT_KEY: $(AZCOPY_E2E_ACCOUNT_KEY)
          AZCOPY_E2E_ACCOUNT_NAME: $(AZCOPY_E2E_ACCOUNT_NAME)
      - script: |
          go build -o test-validator ./testSuite/
          mkdir test-temp
          export AZCOPY_EXECUTABLE_PATH=$(pwd)/azcopy_linux_amd64
          export TEST_SUITE_EXECUTABLE_LOCATION=$(pwd)/test-validator
          export TEST_DIRECTORY_PATH=$(pwd)/test-temp

          # install the CLFSLoad extension
          pip3 install clfsload

          keyctl session test python ./testSuite/scripts/run.py
        name: 'Run_smoke_tests'
        env:
          ACCOUNT_NAME: $(ACCOUNT_NAME)
          ACCOUNT_KEY: $(ACCOUNT_KEY)
          AWS_ACCESS_KEY_ID: $(AWS_ACCESS_KEY_ID)
          AWS_SECRET_ACCESS_KEY: $(AWS_SECRET_ACCESS_KEY)
          GOOGLE_CLOUD_PROJECT: $(GOOGLE_CLOUD_PROJECT)
          GOOGLE_APPLICATION_CREDENTIALS: $(ciGCSServiceAccountKey.secureFilePath)
          ACTIVE_DIRECTORY_APPLICATION_ID: $(ACTIVE_DIRECTORY_APPLICATION_ID)
          AZCOPY_SPA_CLIENT_SECRET: $(AZCOPY_SPA_CLIENT_SECRET)
          CONTAINER_OAUTH_URL: $(CONTAINER_OAUTH_URL)
          CONTAINER_OAUTH_VALIDATE_SAS_URL: $(CONTAINER_OAUTH_VALIDATE_SAS_URL)
          CONTAINER_SAS_URL: $(CONTAINER_SAS_URL)
          FILESYSTEM_SAS_URL: $(FILESYSTEM_SAS_URL)
          FILESYSTEM_URL: $(FILESYSTEM_URL)
          OAUTH_AAD_ENDPOINT: $(OAUTH_AAD_ENDPOINT)
          OAUTH_TENANT_ID: $(OAUTH_TENANT_ID)
          PREMIUM_CONTAINER_SAS_URL: $(PREMIUM_CONTAINER_SAS_URL)
          S2S_DST_BLOB_ACCOUNT_SAS_URL: $(S2S_DST_BLOB_ACCOUNT_SAS_URL)
          S2S_SRC_BLOB_ACCOUNT_SAS_URL: $(S2S_SRC_BLOB_ACCOUNT_SAS_URL)
          S2S_SRC_FILE_ACCOUNT_SAS_URL: $(S2S_SRC_FILE_ACCOUNT_SAS_URL)
          S2S_SRC_S3_SERVICE_URL: $(S2S_SRC_S3_SERVICE_URL)
          S2S_SRC_GCP_SERVICE_URL: $(S2S_SRC_GCP_SERVICE_URL)
          SHARE_SAS_URL: $(SHARE_SAS_URL)
      - script: |
          pip install azure-storage-blob==12.0.0b3
          # the recent release 1.0.0b4 has a breaking change
          pip install azure-core==1.0.0b3
          python ./tool_distributed_mutex.py unlock "$(MUTEX_URL)"
        name: 'Release_the_distributed_mutex'
        # this runs even if the job was canceled (only if the mutex was acquired by this job)
        condition: and(always(), eq(variables['isMutexSet'], 'true'))
